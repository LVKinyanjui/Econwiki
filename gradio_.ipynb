{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is going to be the frontend to interact with our rag application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from pinecone import Pinecone\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01HXCGMB7AARG4J82D47JP7FH0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core.exceptions import FailedPrecondition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01HX56G6P8DTZT2RW97RC1MCJ2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# INITIALIZATION AND CONFIGURATION\n",
    "# Gemini\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "# Pinecone initialization\n",
    "pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'), environment='gcp-starter')\n",
    "index_name = pc.list_indexes()[0]['name']\n",
    "index = pc.Index(index_name)\n",
    "namespace = 'Econwiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01HX56RTFH336ZM4CZFNWYJTXV",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is life?\"\n",
    "def pinecone_query(query: str, with_expansion: bool = False) -> str:\n",
    "    # TODO: Introduce more parameters rather than rely on  hardcoded values\n",
    "    try:\n",
    "        if with_expansion:\n",
    "            system_prompt = f\"\"\"\n",
    "                You are a useful informative assistant\n",
    "                You are to be issued with a question\n",
    "                You will create similar questions \n",
    "                    similar scope, similar topics and themes,\n",
    "                The point is to expound on the original question\n",
    "                    with related questions\n",
    "                No need to write 'Similar questions'\n",
    "                or do extensive formatting\n",
    "\n",
    "                The question is attached after this.\n",
    "\n",
    "            \"\"\"\n",
    "            query = system_prompt + query\n",
    "\n",
    "        query_vector_ = genai.embed_content(content= query,\n",
    "                                            model='models/embedding-001')\n",
    "        query_vector = query_vector_['embedding']\n",
    "\n",
    "        res = index.query(\n",
    "            top_k=5,\n",
    "            vector=query_vector,\n",
    "            include_metadata=True,\n",
    "            namespace=namespace\n",
    "        )\n",
    "\n",
    "        return '\\n\\n'.join([match['metadata']['text'] for match in res['matches']])\n",
    "    \n",
    "    except FailedPrecondition:\n",
    "        # TODO: Raise descriptive gradio error\n",
    "        raise gr.Error(\"Server may be using a VPN. Please disconnect and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01HXCNDKPWJS76R68WAA2ZAV3D",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# # Querying with Expansion\n",
    "# # Used to refine retrieval by generating related queries that may yield a better result\n",
    "# def pinecone_query_with_expansion(query):\n",
    "#     system_prompt = f\"\"\"\n",
    "#         You are a useful informative assistant\n",
    "#         You are to be issued with a question\n",
    "#         You will create similar questions \n",
    "#             similar scope, similar topics and themes,\n",
    "#         The point is to expound on the original question\n",
    "#             with related questions\n",
    "#         No need to write 'Similar questions'\n",
    "#         or do extensive formatting\n",
    "\n",
    "#         The question is attached after this.\n",
    "\n",
    "#     \"\"\"\n",
    "#     augmented_query = system_prompt + query\n",
    "#     res = model.generate_content(augmented_query)\n",
    "#     return res.text\n",
    "\n",
    "# pprint(pinecone_query_with_expansion('What is my name?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01HXCHPSZH3ZAXDCN1MJ50D44D",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_answer_query(context: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "        You are provided with a text to summarize\n",
    "        There may be a main topic or theme that you can identify\n",
    "        Synthesize the various aspects of the texts to create a concise yet informative summary\n",
    "        The text follows below:\n",
    "\n",
    "        {context}\n",
    "\n",
    "    \"\"\"\n",
    "    res = model.generate_content(prompt)\n",
    "    return res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01HX57ACA5ZK8QDM11RSFHFXSB",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 75, in error_remapped_callable\n",
      "    return callable_(*args, **kwargs)\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\grpc\\_channel.py\", line 1161, in __call__\n",
      "    return _end_unary_response_blocking(state, call, False, None)\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\grpc\\_channel.py\", line 1004, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.RESOURCE_EXHAUSTED\n",
      "\tdetails = \"Resource has been exhausted (e.g. check quota).\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:172.217.170.170:443 {created_time:\"2024-06-14T09:51:14.9191042+00:00\", grpc_status:8, grpc_message:\"Resource has been exhausted (e.g. check quota).\"}\"\n",
      ">\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\queueing.py\", line 521, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\blocks.py\", line 1945, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\blocks.py\", line 1513, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\utils.py\", line 831, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9152\\196628524.py\", line 11, in llm_answer_query\n",
      "    res = model.generate_content(prompt)\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\generativeai\\generative_models.py\", line 248, in generate_content\n",
      "    response = self._client.generate_content(request)\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 566, in generate_content\n",
      "    response = rpc(\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\retry.py\", line 366, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\retry.py\", line 204, in retry_target\n",
      "    return target()\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\timeout.py\", line 120, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 77, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    }
   ],
   "source": [
    "# GRADIO APP\n",
    "with gr.Blocks() as demo:\n",
    "    input = gr.Textbox('what is the role of the IMF in Kenya?', label='Query', placeholder='Enter your question here.')\n",
    "    button = gr.Button('Submit')\n",
    "\n",
    "    llm_output = gr.Markdown(label=\"Answer\")\n",
    "    expand_button = gr.Button('Expand Answer', value=True)      # Passes True value to query function to enable expaned query\n",
    "    \n",
    "    with gr.Accordion(\"Retrieved Content\", open=False):\n",
    "        retrieved_output = gr.Markdown()\n",
    "\n",
    "    # button.click(fn=pinecone_query, inputs=[input], outputs=[retrieved_output])\n",
    "    # Multiple triggers\n",
    "    gr.on(triggers=[button.click, input.submit],\n",
    "          fn=pinecone_query,\n",
    "          inputs=[input], \n",
    "          outputs=[retrieved_output],\n",
    "          )\n",
    "    expand_button.click(fn=pinecone_query, inputs=[input, expand_button], outputs=[retrieved_output])\n",
    "    retrieved_output.change(fn=llm_answer_query, inputs=[retrieved_output], outputs=[llm_output])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "1. Query expansion does not appear to be very succesful. It does not extract the document summaries that RAPTOR produces.\n",
    "2. App control flow is such that one can only expand answer once. This should be increased by setting better event listening flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX57YZJSX65AJZCCDDSFGY5D",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
