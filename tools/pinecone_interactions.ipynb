{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pinecone import Pinecone\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "It is important to note that that some models, especially open source ones on hugging face, may have a maximum sequence length. We will therefore limit our `chunk_size=512` to be conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/imf_article_txt\", encoding='utf-8') as f:\n",
    "    texts = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=['_'*150, '\\n\\n', '\\n', '\\t'],\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.split_text(texts)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing (for intfloat/e5-small-v2)\n",
    "Optional modifications to each element in our documents needs to be done to get the most out of the `intfloat/e5-small-v2` model. This involves adding \"query: \" before each string (as this is for similarity search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_ = [f\"query: {document}\" for document in documents]\n",
    "documents_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse and Dense Embeddings\n",
    "We use BM25 for sparse embeddings and hugging face sentence transformers for dense embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = SentenceTransformer(model_dir)\n",
    "except Exception as e:\n",
    "    model = SentenceTransformer('intfloat/e5-small-v2')\n",
    "    print(f\"Caught an error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If model dir empty save model\n",
    "if len(os.listdir(model_dir)) == 0:    \n",
    "    model.save(model_dir)\n",
    "    print(f\"Saved model to {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(\n",
    "    api_key=os.getenv(\"PINECONE_KEY_PROCOPIUS\"),\n",
    "    environment='gcp-starter'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to index\n",
    "# On a starter plan we only have one, so we pick the zero-indexed one\n",
    "# This ensures that even if the name changes, as long as the dimensions don't, we can connect to an index.\n",
    "\n",
    "indexes_info = pc.list_indexes().index_list['indexes']\n",
    "index_name = indexes_info[0]['name']\n",
    "\n",
    "print(f\"The index with name: {index_name}\\nHas a dimension of: {indexes_info[0]['dimension']}\")\n",
    "\n",
    "index = pc.Index(name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed and Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = 'imf-articles'\n",
    "\n",
    "records = []\n",
    "for document in tqdm(documents_):\n",
    "    \n",
    "    dense_vector = model.encode(\n",
    "        document, \n",
    "        normalize_embeddings=True, \n",
    "        # show_progress_bar=True\n",
    "    ).tolist()\n",
    "\n",
    "    sparse_vector = bm25.encode_documents(document)\n",
    "\n",
    "    record = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"values\": dense_vector,\n",
    "        \"sparse_values\": sparse_vector,\n",
    "        'metadata': {\n",
    "            'text': document\n",
    "        }\n",
    "    }\n",
    "\n",
    "    records.append(record)\n",
    "    # index.upsert(record, namespace=namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch and Async Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, batch_size):\n",
    "  return (seq[pos:pos + batch_size] for pos in range(0, len(seq), batch_size))\n",
    "\n",
    "async_results = [\n",
    "  index.upsert(vectors=chunk, namespace=namespace, async_req=True)\n",
    "  for chunk in chunker(records, batch_size=100)\n",
    "]\n",
    "\n",
    "# Wait for and retrieve responses (in case of error)\n",
    "# [async_result.result() for async_result in async_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Pipeline\n",
    "Let us perform a little question answer over our stored documents to ensure it at least works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Has there been staff-level agreements between the IMF and the government of kenya?\"\n",
    "\n",
    "sparse_query = bm25.encode_documents(question)\n",
    "dense_query = model.encode(question).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = index.query(\n",
    "    top_k=3, \n",
    "    vector=dense_query,\n",
    "    sparse_vector=sparse_query,\n",
    "    include_metadata=True,\n",
    "    namespace=namespace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = ''.join([match['metadata']['text'].replace('query: ', '') for match in res['matches']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.getenv(\"PALM_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = f\"\"\"\n",
    "Given the following context :\n",
    "\n",
    "{contexts}\n",
    "\n",
    "try to answer the following question\n",
    "\n",
    "{question}\n",
    "\n",
    "or at least summarize what is contained in the context. all right?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gemini.generate_content(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
